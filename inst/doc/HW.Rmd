---
title: "homework-vignette"
author: "22084"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# HW-0909  
  
## Question

Use knitr to produce at least 3 examples (texts, figures, tables)

## Answer
<p style="font-family: times, serif; font-size:15pt; font-style:normal">
example 1: texts
</p>
This answer is a simple application of Cars93, which is an embedded data package of R. Here are some details.

Cars were selected at random from among 1993 passenger car models that were listed in both the Consumer Reports issue and the PACE Buying Guide. Pickup trucks and Sport/Utility vehicles were eliminated due to incomplete information in the Consumer Reports source. Duplicate models (e.g., Dodge Shadow and Plymouth Sundance) were listed at most once.

<p style="font-family: times, serif; font-size:15pt; font-style:normal">
example 2: figures
</p>

Here we use the Manufacturer/EngineSize/Price data to plot figures below as example 2
```{r}
library(MASS)
data("Cars93")
plot(Cars93$Manufacturer)
plot(Cars93$EngineSize)
hist(Cars93$Price,col='pink',xlab='price/1k ',main='Cars93 price')
```

<p style="font-family: times, serif; font-size:15pt; font-style:normal">
example 3: tables
</p>

Using the first 12 rows cross the first 6 columns of Cars93 to demonstrate example 3
```{r}
knitr::kable(Cars93[1:12,1:6])
rmarkdown::paged_table(Cars93[1:18,1:6])
```
      
      
# HW-09-15  
       
       
## Question 3.3  
<p style="font-family: times, serif; font-size:18pt; font-style:normal">

The Pareto(a, b) distribution has cdf     
$$F(x)=1-(\frac{b}{x})^a,\qquad x\ge b>0,a>0.$$   
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse 
transform method to simulate a random sample from the Pareto(2, 2) distribution.
Graph the density histogram of the sample with the Pareto(2, 2) density
superimposed for comparison.
</p>

## Answer  
<p style="font-family: times, serif; font-size:15pt; font-style:normal">
inverse transformation is $x=b(1-u)^{-\frac{1}{a}}$
</p>
```{r echo=FALSE}
a<-2;b<-2;n=100
f<-function(x){
  (a/b)*(b/x)^(a+1)
}
inverse_F<-function(u){
  b*(1-u)^(-1/a)
}
u<-runif(n)
x<-inverse_F(u)
hist(x,breaks=60,freq = FALSE,xlim=c(0,20),ylim=c(0,1.2),ylab=NULL)
par(new=TRUE)
curve(f,2,30,xlim = c(0,20),ylim=c(0,1.2),ylab=NULL,col="blue")

```  
  
## Question 3.7
<p style="font-family: times, serif; font-size:15pt; font-style:normal">
Write a function to generate a random sample of size n from the Beta(a, b)
distribution by the acceptance-rejection method. Generate a random sample
of size 1000 from the Beta(3,2) distribution. Graph the histogram of the
sample with the theoretical Beta(3,2) density superimposed
</p>


## Answer
```{r echo=FALSE}
a<-3;b<-2
f<-function(x){
  1/beta(a,b)*x^(a-1)*(1-x)^(b-1)
}
c=f((a-1)/(a+b-2))

n <- 1000;k<-0;sample <- numeric(n)
while (k < n) {
  u <- runif(1)
  x <- runif(1) 
  if (dbeta(x,3,2)/c > u) {
    k <- k + 1 
    sample[k] <- x
  }
}  
hist(sample,xlim=c(0,1),ylim=c(0,2),breaks = 50,freq = FALSE,ylab = NULL,xlab = NULL)
par(new=TRUE)
curve(f,ylim=c(0,2),col='red',ylab=NULL)
abline(h=c,col='pink')


```  
  
## Question 3.12  
<p style="font-family: times, serif; font-size:15pt; font-style:normal">
  Simulate a continuous Exponential-Gamma mixture. Suppose that the rate
parameter Λ has Gamma(r, β) distribution and Y has Exp(Λ) distribution.
That is, $(Y |Λ = λ) ∼ f_y(y|λ) = λe^{−λy}$. Generate 1000 random observations
from this mixture with r = 4 and β = 2. 
</p>
 
## Answer  
```{r echo=FALSE}
n <- 1000; r <- 4; beta <- 3
lambda <- rgamma(n, r, beta)
x <- rexp(n,lambda)
x<-data.frame(x)
rmarkdown::paged_table(x)
```  
  
## Question 3.13  

<p style="font-family: times, serif; font-size:15pt; font-style:normal">
It can be shown that the mixture in Exercise 3.12 has a Pareto distribution
with cdf
$$F(y)=1-(\frac{\beta}{\beta +y})^r,\quad y\ge 0.$$
(This is an alternative parameterization of the Pareto cdf given in Exercis
3.3.) Generate 1000 random observations from the mixture with r = 4 and
β = 2. Compare the empirical and theoretical (Pareto) distributions by graphing
the density histogram of the sample and superimposing the Pareto density curve.
</p>

## Answer
```{r echo=FALSE}
n <- 1000; r <- 4; beta <- 2
lambda <- rgamma(n, r, beta)
x <- rexp(n,lambda)
hist(x,breaks = 50,xlim=c(0,15),ylim=c(0,1.3),freq=FALSE,ylab = NULL)

f<-function(y){
  (r/beta)*(beta/(beta+y))^(r+1)
}
par(new=TRUE)
curve(f,xlim = c(0,15),ylim=c(0,1.3),col='red')
```
  
   
   
# HW-0923   
     

##  Question 3.3  
<p style="font-family: times, serif; font-size:20pt; font-style:normal">
   
### Q1
  
For n = 104, 2 × 104, 4 × 104, 6 × 104, 8 × 104, apply the fast sorting algorithm to randomly permuted numbers of 1, . . . , n.  

### Answer   
```{r,echo=FALSE}
n<-c(10^4,2*10^4,4*10^4,6*10^4,8*10^4)
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}
}
test<-lapply(1:length(n), function(i)sample(1:n[i]))
#rmarkdown::paged_table(data.frame(test[[1]]))
sorted_test<-lapply(1:length(test),function(i)quick_sort(test[[i]]))
```  
The application of the fast sorting algorithm when n = 10000:
```{r,echo=FALSE}
rmarkdown::paged_table(data.frame('Original_list'=test[[1]],'Sorted_list'=sorted_test[[1]]))
```  
The application of the fast sorting algorithm when n = 20000:
```{r,echo=FALSE}
rmarkdown::paged_table(data.frame('Original_list'=test[[2]],'Sorted_list'=sorted_test[[2]]))
```  
The application of the fast sorting algorithm when n = 40000:
```{r,echo=FALSE}
rmarkdown::paged_table(data.frame('Original_list'=test[[3]],'Sorted_list'=sorted_test[[3]]))
```  
The application of the fast sorting algorithm when n = 60000:
```{r,echo=FALSE}
rmarkdown::paged_table(data.frame('Original_list'=test[[4]],'Sorted_list'=sorted_test[[4]]))
```  
The application of the fast sorting algorithm when n = 80000:
```{r,echo=FALSE}
rmarkdown::paged_table(data.frame('Original_list'=test[[5]],'Sorted_list'=sorted_test[[5]]))
```  

### Q2
  
computation time averaged over 100 simulations, denoted by $a_n$.  
     
### Answer  
```{r,echo=FALSE}
a<-numeric(length(n))
sim_num<-100 # the number of simulations for each n 
for(i in 1:length(n)){
  t_j<-sapply(1:sim_num,function(j){# For each n, we did 100 times simulation.
    test<-sample(1:n[i])
    return(system.time(quick_sort(test))[[1]])
  })# All the time costs of 100 simulations formed vector t_j
  a[i]<-mean(t_j) 
}
for(i in 1:length(n)){
  cat('\nThe computation time averaged over 100 simulations of n =',n[i],'is',a[i])
}
```  
    
### Q3
  
Regress an on tn := n log(n), and graphically show the results (scatter plot and regression line).
    
### Answer   
```{r,echo=FALSE}
t<-sapply(n,function(i)i*log(i))
my_lm<-lm(t~a)
plot(a,t,main='a~t regression')
abline(my_lm,lwd=2,col='red')
```  
   
## Question 5.6   
In Example 5.7 the control variate approach was illustrated for Monte Carlo integration of   
$$\theta=\int_0^1e^xdx$$  
Now consider the antithetic variate approach. Compute $Cov(e^U,e^{1−U})$ and $Var(e^U+e^{1−U})$, where U ∼ Uniform(0,1). What is the percent reduction in variance of ˆθ that can be achieved using antithetic variates (compared with simple MC)?   
    
## Answer   
\begin{align*}
cov(e^U,e^{1−U})=& E[(e^u-E(e^u))(e^{1-u}-E(e^{1-u}))]\\
                =& E(e^u*e^{1-u})-(1-e)^2\\
                =& e-(1-e)^2\\
                =& 3e-1-e^2\\
                \\
var(e^U+e^{1−U})=& var(e^u)+var(e^{1-u})+2cov(e^U,e^{1−U})\\
                =& e^2-1-2(1-e)^2+2(3e-1-e^2)\\
                =& -5+10e-3e^2\\
                
\end{align*}  

   

Assume m samples is sampled for simple MC and m/2 samples is sampled for antithetic variate approach,
compared with simple MC, then we have: 
\begin{align*} 
The\ percent\ reduction\ in\ variance\ of\ \hat{\theta}\ is\ 
p=&1-\frac{var(\hat{\theta}_{mc2})}{var(\hat{\theta}_{mc1})}\ where\\
\hat{\theta}_{mc2}=&\frac{1}{m}\sum_{i=1}^{\frac{m}{2}}(e^{u_i}+e^{(1-u_i)})\\  
\hat{\theta}_{mc1}=&\frac{1}{m}\sum_{i=1}^me^{u_i}\\
var(\hat{\theta}_{mc2})=&\frac{1}{m^2}.\frac{m}{2}var(e^{u}+e^{(1-u)})\\
=&\frac{1}{2m}var(e^{u}+e^{(1-u)})\\
var(\hat{\theta}_{mc1})=&\frac{1}{m}var(e^u)\\
so\ the\ percent\ reduction\ in\ variance\ of\ \hat{\theta}\ is :\\
p=1-\frac{var(e^u+e^{1−u})}{2var(e^u)}=&1-\frac{-5+10e-3e^2}{4e-e^2-3}=\frac{2-6e+2e^2}{4e-e^2-3}=0.9676701
\end{align*}   
  
   
## Question 5.7   
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate θ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value from Exercise 5.6.  
```{r,echo=FALSE}
MC.Phi <- function(R = 1, antithetic = FALSE) {
  u <- runif(R)
  if (antithetic){
    v <- 1 - u 
    u <- c(u, v)
  } 
  g <- exp(u)
  cdf <- mean(g) 
  cdf
}
m <- 1000
MC1 <- MC2 <- numeric(m) #MC2[i]=(exp(u)+exp(1-u))/2; MC1[i]=exp(u)
for (i in 1:m) {
  MC1[i] <- MC.Phi(R = 1, antithetic = FALSE)
  MC2[i] <- MC.Phi(R = 1, antithetic = TRUE)
}
p<-1-2*var(MC2)/var(MC1) 
print(p)
```    
   
## Answer
   
Result with the theoretical value from Exercise 5.6 is 0.967670, while here we get an empirical estimate of the percent reduction in variance as above.   
    
                    
      
</p>    
   
    
# HW-0930

## Question 5.13   
Find two importance functions f1 and f2 that are supported on (1, ∞) and
are ‘close’ to  
$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},\ \ \ x>1.$$  
Which of your two importance functions should produce the smaller variance
in estimating  
$$\int_1^\infty\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx$$  
by importance sampling? Explain.
  
  
## Answer  
  
Considering that Gamma(2,0.5) distribution has a density function as below:
$$f(x)=\frac{1}{2^3\Gamma(3)}x^2e^{-x/2}$$  
let $p=\int_0^1f(x)dx$, we get a density function $f_1$ supported on $(0,\infty)$
$$f_1(x)=\frac{1}{1-p}f(x)=\frac{1}{16(1-p)}x^2e^{-x/2}$$  
By curving function g(x), we can find it's alike the density of $\lambda$ distribution:  
```{r,echo=FALSE}
g<-function(x){
  x^2*exp(-x^2/2)/sqrt(2*pi)
}
curve(g,1,10)
```  
   
   
# HW-1009
  
## Question 6.4   
 Suppose that X1,...,Xn are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter µ. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.
  
## Answer  



$$X\sim N(\mu,\sigma ^2)$$
$$\bar{X}\sim N(\mu,\sigma^2/n)$$
$$\frac{\sqrt{n}(\bar{X}-\mu)}{S}\sim t_{n-1} $$ 
$$Where \ \ S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X}) $$ 
$$So\   P(t_{n-1}(0.025)<\frac{\sqrt{n}(\bar{X}-\mu)}{S}<t_{n-1}(0.975))=0.95 $$ 
$$That\  is\  P(\bar{X}-S·t_{n-1}(0.975)/\sqrt{n}<\mu<\bar{X}-S·t_{n-1}(0.025)/\sqrt{n})=0.95 $$ 
$$In\  this\ case, we\ get\ a\ 95\% \ confidence\ interval\ for\ the\ parameter\ \mu:  $$
$$[\bar{x}-\hat{S}·t_{n-1}(0.975)/\sqrt{n},\ \bar{x}-\hat{S}·t_{n-1}(0.025)/\sqrt{n}]$$  

Use a Monte Carlo method to obtain an empirical estimate of the confidence level:


```{r}
set.seed(12)
n<-10;m<-10000
mu<-0;sigma<-1
low<-high<-numeric(m) # left end point and right end point
for(i in 1:m){
  x<-rnorm(n,mu,sigma)
  x_bar<-mean(x)
  S<-sd(x)
  low[i]<-x_bar-S*qt(0.975,n-1)/sqrt(n)
  high[i]<-x_bar-S*qt(0.025,n-1)/sqrt(n)
}
mean((0>low)*(0<high)) #empirical estimate of the confidence level

```
The empirical estimate of the confidence level is 0.9495  


## Question 6.8
Refer to Example 6.16. Repeat the simulation, but also compute the F test of equal variance, at significance level ˆα = 0.055. Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the F test is not applicable for non-normal distributions.)
## Answer  
Repeat the simulation for  small=10, medium=100, and large sample sizes=1000

```{r}
set.seed(123)
n<-c(10,100,1000)
m<-10000
sigma1 <- 1
sigma2 <- 1.5
# generate samples under H1 to estimate power
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}
for (i in 1:3){
  power <- mean(replicate(m, expr={
    x <- rnorm(n[i], 0, sigma1)
    y <- rnorm(n[i], 0, sigma2)
    count5test(x, y)
    }))
  print(power)
}




```  
under $H_0$, we have $n_1=n_2=19,\ \sigma_1=\sigma_2$
$$F=\frac{\frac{(n_1-1)S^2_1}{\sigma_1^2}/(n_1-1)}{\frac{(n_2-1)S^2_2}{\sigma_2^2}/(n_2-1)}=S^2_1/ S_2^2\sim F_{19,19}$$  
F test for  small=10, medium=100, and large sample sizes=1000:  
```{r}
alpha<-0.055
set.seed(123)
F_hat<-numeric(m)
for(i in 1:3){
  for(j in 1:m){
  x <- rnorm(n[i], 0, sigma1)
  y <- rnorm(n[i], 0, sigma2)
  F_hat[j] = var(x)/var(y)
  }
  low<-qf(alpha/2,n[i]-1,n[i]-1);high<-qf(1-alpha/2,n[i]-1,n[i]-1) # left end point and right end point of accept interval 
  print(1-mean((F_hat<high)*(F_hat>low))) # empirical estimate of power
}

```  


## Question 3
 If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments:
say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05
level?
• What is the corresponding hypothesis test problem?
• Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?
• Please provide the least necessary information for hypothesis testing.


## Answer3.1  

$H_0$:power1 equals to power2; $H_1$:the powers are different.  
The significant level is 0.05.

## Answer3.2
   We can use two-sample test.  
     
   Since every single sample used to computer power1(power2) is binomial distributed with rejection probability = power1(power2)>0.5(with high probability,since the empirical number is 0.651,0.676).So the empirical power1 and empirical power2 both are limit normal distribution.  
      
      
   Here the number of simulation is m=10000. We can absolutely take the empirical powers as Normal dist.Then under $H_0$, we have X(power1) and Y(power2)$\sim N(\mu,\sigma^2)$. In this case:  
$$\frac{\sqrt{\frac{n_1n_2}{n_1+n_2}}(\bar{X}-\bar{Y})}{S_{X+Y}}\sim t_{n_1+n_2-2}$$  
As for this problem,we only get one sample for X and Y. So the statistic is:  
$$\frac{(X-Y)/ \sqrt{2}}{S_{X+Y}}\sim t_{n_1+n_2-2}$$  
Here $S_{X+Y}$ is computed using n1 samples of X and n2 samples of Y which are not given.  


## Answer3.3
From answer3.2 above, we want to use two sample t_test. But we need $S_{X+Y}$ which is computed using n1 samples of X and n2 samples of Y.

  
  
# HW-1014
  
## Question 7.4   
 Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of airconditioning equipment [63, Example 1.1]: 3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487. Assume that the times between failures follow an exponential model Exp(λ). Obtain the MLE of the hazard rate λ and use bootstrap to estimate the bias and standard error of the estimate.
  
## Answer  
$$the\ joint\ pdf\ is\ \ f(x_1,x_2,...,x_{12}|\lambda)=\lambda^{12}e^{-\lambda(x_1+x_2+...+x_{12})}$$
$$loglikehood\ is\ \ \ l(\lambda|x_1,x_2,...,x_{12})=12log(\lambda)-\lambda(x_1+x_2+...+x_{12}))$$

$$let\ \ \  \frac{\partial l}{\partial \lambda}=\frac{12}{\lambda}-(x_1+x_2+...+x_{12})=0$$
$$we\ get\ \ \ \lambda_0=\frac{12}{(x_1+x_2+...+x_{12})}$$
$$that\ is\ MLE\ of\ \lambda= \frac{12}{(x_1+x_2+...+x_{12})}=0.00925212$$  

```{r}
library(boot)
x<-c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
stat<-function(x,i){1/mean(x[i])}
lambda_mle<-12/sum(x)
obj<-boot(data=x,statistic = stat,R=99)
round(c(original=obj$t0,bias=mean(obj$t)-obj$t0,se=sd(obj$t)),6)
```  

## Question 7.5
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures 1/λ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer
```{r}
ci<- boot.ci(obj,type=c("norm","basic","perc",'bca'))
cat('normal:\n' ,ci$norm[2:3],'\n basic:\n',ci$basic[4:5], '\n percentile:\n',ci$percent[4:5],'\n BCa:\n',ci$bca[4:5] )
```    
The difference may caused by different distributions of statistic corresponding to each methods.  

## Question 7.A 
Conduct a Monte Carlo study to estimate the coverage probabilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample mean. Find the proportion of times that the confidence intervals miss on the left, and the proportion of times that the confidence intervals miss on the right.
```{r}
set.seed(1234)
mu<-0
n<-10;m<-100
ci.norm<-ci.basic<-ci.perc<-ci.bca<-matrix(NA,m,2)
stat<-function(x,i)mean(x[i])
for(i in 1:m){
  x<-rnorm(n,mu)
  obj <- boot(data=x,statistic=stat, R = 99)
  ci <- boot.ci(obj,type=c("norm","basic","perc"))
  ci.norm[i,]<-ci$norm[2:3]
  ci.basic[i,]<-ci$basic[4:5]
  ci.perc[i,]<-ci$percent[4:5]
}
cat('proportion that CI miss on the left:  norm=',mean(ci.norm[,1]>=mu),'basic =',mean(ci.basic[,1]>=mu ),'perc =',mean(ci.perc[,1]>=mu))
cat('proportion that CI miss on the right:  norm =',mean(ci.norm[,2]<=mu ),'basic =',mean(ci.basic[,2]<=mu ),'perc =',mean(ci.perc[,2]<=mu))

```

  
    
# HW-1021 
  
## Question 7.8  
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of ˆθ.  

```{r}
library(bootstrap)
n=dim(scor)[1]
theta.jack<-numeric(n)
lambda<-eigen(cor(scor))$val
theta.hat<-lambda[1]/sum(lambda)
for(i in 1:n){
  jack_df<-scor[-i,]
  Sigma<-matrix(nrow = 5,ncol=5)
  for(j in 1:5){
    for(k in j:5){
      Sigma[j,k]=Sigma[k,j]=cor(jack_df[,k],jack_df[,j])
    }
  }
  lambda_hat<-eigen(Sigma)$val
  theta.jack[i]<-lambda_hat[1]/sum(lambda_hat)
}
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))
round(c(bias.jack=bias.jack,
se.jack=se.jack),6)

```  


  
## Question 7.11  
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.
```{r}
library(DAAG)
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n*(n-1)/2)
i=0
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:(n-1)) {
  for(j in (k+1):n){
    i=i+1
    y <- magnetic[-c(k,j)]
    x <- chemical[-c(k,j)]
    J1 <- lm(y ~ x)
    yhat1 <- J1$coef[1] + J1$coef[2] * chemical[c(k,j)]
    e1[i]<- sum((magnetic[c(k,j)] - yhat1)^2)
    
    J2 <- lm(y ~ x + I(x^2))
    yhat2 <- J2$coef[1] + J2$coef[2] * chemical[c(k,j)] +
    J2$coef[3] * chemical[c(k,j)]^2
    e2[i]<- sum((magnetic[c(k,j)] - yhat2)^2)
    J3 <- lm(log(y) ~ x)
    logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[c(k,j)]
    yhat3 <- exp(logyhat3)
    e3[i]<- sum((magnetic[c(k,j)] - yhat3)^2)
    J4 <- lm(log(y) ~ log(x))
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[c(k,j)])
    yhat4 <- exp(logyhat4)
    e4[i]<- sum((magnetic[c(k,j)] - yhat4)^2)
  }
}
round(c(mean_e1=mean(e1), mean_e2=mean(e2), mean_e3=mean(e3), mean_e4=mean(e4)),3)
```  
  
  
## Question 8.2  
Implement the bivariate Spearman rank correlation test for independence
[255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the
achieved significance level of the permutation test with the p-value reported
by cor.test on the same samples.  
```{r}
n=20
set.seed(123)
x<-rnorm(n,1,2)
y<-runif(n,2,10)
R <- 999;z <- c(x, y);K <- 1:(2*n)
reps <- numeric(R);t0 <- cor(x, y,method='spearman')
for (i in 1:R) {
  k <- sample(K, size = n, replace = FALSE)
  x1 <- z[k]; y1 <- z[-k] #complement of x1
  reps[i] <- cor(x1, y1,method='spearman')
}
p <- mean(abs(c(t0, reps)) >= abs(t0))
round(c(permutation_p=p,cor.test_p=cor.test(x,y)$p.value),3)
```
Under $H_0$:X,Y are independent.  
By permutation test,we have p_value=0.160, p_value reported by cor.test is 0.119

  
# HW-1028
  
## Question 9.4  
  
Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.


## Answer
```{r}
rw.Metropolis <- function(sigma, x0, N){ 
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (exp(-abs(y) )/exp(-abs(x[i-1])) ))
      x[i] <- y else {
        x[i] <- x[i-1]
        k <- k + 1
      }
    }
return(list(x=x, k=k))
}

Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi) #row means
  B <- n * var(psi.means) #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w) #within est.
  v.hat <- W*(n-1)/n + (B/n) #upper variance est.
  r.hat <- v.hat / W #G-R statistic 
  return(r.hat)
  
}

set.seed(123)
sigma <- c(1,2,3,4) #different sigma
k <- 4 #number of chains to generate for each sigma
n <- 6000 #length of chains
b <- 500 #burn-in length
#choose overdispersed initial values for each sigma
x0 <- c(-10, -5, 5, 10)
#generate the chains
S <- list() #generate a list,each element corresponds to different sigma and contains 4 chains 
R <- numeric(length(sigma)) # R
accept_rate <- matrix(0,length(sigma),k)
for(j in 1:length(sigma)){
  
  X <- matrix(0, nrow=k, ncol=n)
  for (i in 1:k){
    results <- rw.Metropolis(sigma[j], x0[i], n)
    X[i, ] <- results[[1]] 
    accept_rate[j,i] <- (n+b-results[[2]])/(n+b)
  }
  #compute diagnostic statistics
  psi <- t(apply(X, 1, cumsum))
  for (i in 1:nrow(psi)){
    psi[i,] <- psi[i,] / (1:ncol(psi))
  }
  S[[j]] <- psi #save diagnostic stat from each sigma as a matrix
  R[j] <- Gelman.Rubin(psi)
}


#plot the sequence of R-hat statistics
for(i in 1:length(sigma)){
  psi<-S[[i]]
  rhat <- rep(0, n)
  for (j in 1:n) rhat[j] <- Gelman.Rubin(psi[,1:j])
  plot(rhat[b:n], type="l", xlab="", ylab="R",main=bquote(sigma ==.(sigma[i])))
  abline(h=1.2, lty=2)
}

```     
  

  
# HW-1105  
  

## Question1  

```{r}
#data prepare
set.seed(12345)
n<-20
x<-runif(n,1,10) 
alpha<-c(0,0,1)
beta<-c(0,1,0)
lambda<-rep(1,3)
paramas<-cbind(alpha,beta,lambda)
a_m<-a_y<-1

#linear model 
#simulate M&Y
M<-a_m+x%*%t(alpha)+rnorm(n*3,0,0.1)
Y<-a_y+M%*%diag(beta)+x%*%t(lambda)+rnorm(n*3,0,0.1)
```  


1)For H0: alpha=0, for each of the three paramates vectors (alpha,beta)=(0,0)||(0,1)||(1,0), use permutation methond to compute p-value of this test.
```{r}
#First: for H0: a=0
###for each of the three paramates vectors (alpha,beta)
###use permutation method to compute p-value of this test.
p<-numeric(3)
R <- 999;K <- 1:(2*n)
for(i in 1:3){
  alpha_hat<-numeric(R)
  my_fit<-lm(M[,i]~x)
  my_fit<-summary(my_fit)
  alpha_0<-my_fit$coefficients[2,1]
  z <- c(M[,i], x)
  for (j in 1:R) {
    k <- sample(K, size = n, replace = FALSE)
    Mj<- z[k]; xj<- z[-k] 
    my_fit<-lm(Mj~xj)
    my_fit<-summary(my_fit)
    alpha_hat[j]<-my_fit$coefficients[2,1]
  }
  p[i]<-mean(abs(c(alpha_0, alpha_hat)) >= abs(alpha_0))
}
```
Under this H0:alpha=0,for (alpha,beta)=(0,0)||(0,1)||(1,0),we can get p-values for each of them as below.  
```{r}
round(p,3)
```
As you can see, the results are identical with H0: alpha·beta=0 on situation of (alpha,beta)=(0,0)||(0,1),which successfully accept alpha=0. However,when it comes to (alpha,beta)=(1,0),the Type I error is 0.001, which is absolutely less than Type Ie of H0: alpha·beta=0. There is no need to calculate the specific number of Type Ie of H0: alpha·beta=0, as (alpha,beta)=(1,0) will be accepted under this H0,which means p>=0.01.
  
  
2)For H0: beta=0, for each of the three paramates vectors (alpha,beta)=(0,0)||(0,1)||(1,0), use permutation methond to compute p-value of this test.
```{r}
#First: for H0: a=0
###
###

for(i in 1:3){
  beta_hat<-numeric(R)
  my_fit<-lm(Y[,i]~M[,i]+x)
  my_fit<-summary(my_fit)
  beta_0<-my_fit$coefficients[2,1]
  z <- c(M[,i], Y[,i])
  for (j in 1:R) {
    k <- sample(K, size = n, replace = FALSE)
    Yj<- z[k]; Mj<- z[-k] 
    my_fit<-lm(Yj~Mj+x)
    my_fit<-summary(my_fit)
    beta_hat[j]<-my_fit$coefficients[2,1]
  }
  p[i]<-mean(abs(c(beta_0, beta_hat)) >= abs(beta_0))
}
round(p,3)
```  
As you can see, the results are identical with H0: alpha·beta=0 on situation of (alpha,beta)=(0,0)||(1,0),which successfully accept beta=0. However,when it comes to (alpha,beta)=(0,1),the Type I error is 0.001, which is absolutely less than Type Ie of H0: alpha·beta=0. There is no need to calculate the specific number of Type Ie of H0: alpha·beta=0, as under situation (alpha,beta)=(0,1),this H0 will be accepted,which means p>=0.01.  
  
3)For H0: alpha=beta=0
```{r}
#First: for H0: a=0
###for each of the three paramates vectors (alpha,beta,lambda)
###use permutation method to compute p-value of this test.

for(i in 1:3){
  alpha_hat<-beta_hat<-numeric(R)
  my_fit<-lm(Y[,i]~M[,i]+x)
  my_fit<-summary(my_fit)
  beta_0<-my_fit$coefficients[2,1]
  alpha_hat<-numeric(R)
  my_fit<-lm(M[,i]~x)
  my_fit<-summary(my_fit)
  alpha_0<-my_fit$coefficients[2,1]
  z1 <- c(M[,i], Y[,i])
  z2 <- c(M[,i], x)
  for (j in 1:R) {
    k <- sample(K, size = n, replace = FALSE)
    Yj<- z1[k]; Mj<- z1[-k]
    Mj2<-z2[k]; xj<- z2[-k]
    my_fit1<-lm(Yj~Mj);my_fit2<-lm(Mj2~xj)
    my_fit1<-summary(my_fit1);my_fit2<-summary(my_fit2)
    beta_hat[j]<-my_fit1$coefficients[2,1]
    alpha_hat[j]<-my_fit2$coefficients[2,1]
  }
  c<-(abs(c(beta_0, beta_hat)) >= abs(beta_0))*(abs(c(alpha_0, alpha_hat)) >= abs(alpha_0))
  p[i]<-mean(c)
}
round(p,3)
```  
It's simply alike the conclusions made above, no need to repeat. In conclusion,all the three H0 is not enough to cover the Type I error of ordinary H0:alpha·beta=0.


## Question 2
1)
```{r}
solve_alpha<-function(N,b1,b2,b3,f0){ 
  x1 <- rpois(N,1); x2 <- rexp(N);x3<-rbinom(N,1,0.5)
  g <- function(alpha){
    tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
    p <- 1/(1+tmp)
    mean(p) - f0
  }
  solution <- uniroot(g,c(-20,20))
  return(solution$root)
}
```
  
2)
```{r}
N <- 1e6; b1 <- 0; b2 <- 1; b3 <- -1
f0<-c(0.1,0.01,0.001,0.0001)
alpha<-numeric(4)
for (i in 1:4){
  alpha[i]<-solve_alpha(N,b1,b2,b3,f0[i])
}
plot(alpha,f0)
```  
  
   
# HW-1111
         
           
## Question 1 class work


## Answer 1.1  
Proof:  

First we derive MLE estimate.
$$\$$
\begin{equation}

L(\lambda)=\prod_{i=1}^nP_{\lambda}(u_i<x<v_i)=\prod_{i=1}^n(e^{-\lambda u_i}-e^{-\lambda v_i})\\
l(\lambda)=\sum_{i=1}^nlog(e^{-\lambda u_i}-e^{-\lambda v_i})\\
\frac{\partial l}{\partial \lambda}=\sum_{i=1}^n \frac{-u_i e^{-\lambda u_i}+v_i e^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}\\
we\ get\ MLE\ \lambda_{mle}\ as\ the\ root\ of\  \frac{\partial l}{\partial \lambda}=0 ------(1)\\


\end{equation}

Then we derive the EM method.
\begin{equation}

l(\lambda|x_1,x_2,...,x_n)=n log(\lambda)-\lambda \sum_{i=1}^n x_i\\
E_{\lambda_0}(l(\lambda |x_1,x_2,...,x_n)) =n log(\lambda)-\lambda \sum_{i=1}^n E_{\lambda_0}(x_i|(u_i,v_i))= n log(\lambda)-\lambda \sum_{i=1}^n \int_{u_i}^{v_i}x\frac{\lambda_0 e^{-\lambda_0 x}}{e^{-\lambda_0 u_i}-e^{-\lambda_0 v_i}} \\
Let\ \frac{\partial E_{\lambda_0}(l(\lambda|x_1,x_2,...,x_n)) }{\partial \lambda}=0\\
we\ get\ \lambda^{'}=\frac{n}{\frac{n}{\lambda_0}+\frac{sum_{i=1}^n(-u_i e^{-\lambda_0 u_i}+v_i e^{-\lambda_0 v_i})}{e^{-\lambda_0 u_i}-e^{-\lambda_0 v_i}}}\\

So\ \lambda_{EM}\ is\ the\ root\ of\ function\ \lambda=\frac{n}{\frac{n}{\lambda}+\frac{sum_{i=1}^n(-u_i e^{-\lambda u_i}+v_i e^{-\lambda v_i})}{e^{-\lambda u_i}-e^{-\lambda v_i}}}------(2)\\

\end{equation}   

However,with function (1),(2), we find $\lambda_{mle}$ happens to be the root of the function above. which means $\lambda_{mle}=\lambda_{EM}$ 


## Answer 1.2
```{r}

u<-c(11,8,27,13,16,0,23,10,24,2)
v<-c(12,9,28,14,17,1,24,11,25,3)
```

MLE:  
```{r}
# log likehood function
l<-function(lambda){
  log(prod(exp(-lambda*u)-exp(-lambda*v)))
}
f<-function(x){
  x^2-9
}
solve<-optimize(l,lower=0,upper=1,maximum = TRUE)
lambda_mle<-solve$maximum
round(lambda_mle,3)
```   

   
   
   
   
   
EM： 
```{r}
it <- 7000  
n<-length(u)
lambda_em<-numeric(it)  
lambda_em[1]<-1  
for(i in 2:it){  
  l<-(u*exp(-lambda_em[i-1]*u)-v*exp(-lambda_em[i-1]*v))/(exp(-lambda_em[i-1]*u)-exp(-lambda_em[i-1]*v))
  s<-sum(l)
  lambda_em[i]<-n/(n/lambda_em[i-1]+s)  
}  
round(lambda_em[it],3)
```     
From the results of MLE method and EM method, we can find it's nearly same.  

  
## Question 2.1.3.4  
Why do you need to use unlist() to convert a list to an atomic
vector? Why doesn’t as.vector() work?

```{r}
l<-list(1,2,3)
a=as.vector(l)
print(unlist(l))
typeof(l[1])
```  
## Answer 
The elements of list may have different modes.Besides,the type of element in a list is also list, so when we use function <as.vector> trying to convert l to a vector, it won't work as what we expected.
  
  
## Question 2.1.3.5
Why is 1 == "1" true? Why is -1 < FALSE true? Why is "one"
< 2 false?  
 
```{r}
print(1=='1')
print(-1<FALSE)
print('one'<2)
```  

## Answer  
  
When be compered,a string is transformed to numeric mode,such as '1' is 1. 'one' is transformed to numeric mode too, however it's bigger than 2. FALSE is 0,TRUE is 1.


## Question 2.3.1.2  
What does dim() return when applied to a vector?

```{r}
v<-c(1,2,3)
dim(v)
```  
## Answer  

The result is Null  
  
##  Question 2.3.1.3
How would you describe the following three objects? What makes them different to 1:5?    
x1 <- array(1:5, c(1, 1, 5))  
x2 <- array(1:5, c(1, 5, 1))  
x3 <- array(1:5, c(5, 1, 1))    

## Answer  
x1 is a matrix with 1x1x5 dim, x2 is a matrix with 1x5x1 dim, x3 is a matrix with 5x1x1 dim. 1:5 is simply a vector with length 5.


 
## Question 2.4.5.1  
What attributes does a data frame possess ?  

```{r}
df <- data.frame(x = 1:3, y = c("a", "b", "c"))
names(attributes(df))
```
## Question 2.4.5.2  
```{r}
df <- data.frame(x = 1:3, y = c("a", "b", "c"))
as.matrix(df)
```  

## Answer  
As you can see,it will convert the type of different columns to the same type.

  
## Question 2.4.5.3  
Can you have a data frame with 0 rows? What about 0 columns?  

## Answer  
```{r}
df<-data.frame()
dim(df)
```    
Here we have a dataframe with 0 rows and 0 columns.  
   
   
# HW-18  

## Question 11.1.2.2  
The function below scales a vector so it falls in the range [0,
1]. How would you apply it to every column of a data frame?
How would you apply it to every numeric column in a data
frame?  
scale01 <- function(x) {  
  rng <- range(x, na.rm = TRUE)  
  (x - rng[1]) / (rng[2] - rng[1])  
}  
  
## Answer  
```{r}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
X<-data.frame(x=6:10,y=1:5,z=c('a','b','c','d','e'))
print(X)
apply(X[,sapply(X,is.numeric)], 2, scale01)
```  

## Question 11.2.5.1  
1. Use vapply() to:  
a) Compute the standard deviation of every column in a numeric data frame.  
b) Compute the standard deviation of every numeric column
in a mixed data frame. (Hint: you’ll need to use vapply()  
twice.)  


## Answer a)
```{r}
X<-data.frame(matrix(runif(40,1,10),ncol = 5))
X
vapply(X, sd, numeric(1))
```    


## Answer b)
```{r}
X<-data.frame(matrix(runif(24,1,10),ncol = 3),z=strsplit('abcdefgh',split = ''))
colnames(X)<-c(1,2,3,4)
vapply(X[,vapply(X, is.numeric, logical(1))], sd, numeric(1))  
```  



## Question 3   
   Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt) with zero means, unit standard deviations, and correlation 0.9.  
• Write an Rcpp function.  
• Compare the corresponding generated random numbers with pure R language using the function
“qqplot”.  
• Compare the computation time of the two functions with the function “microbenchmark”.   

## Answer 1)
```{r}
library(Rcpp)
sourceCpp('../src/StatCompC.cpp')
N=5000;mu1=0;mu2=0;sigma1=1;sigma2=1;rho=0.9
xy_cpp<-cppgibbs(N,mu1,mu2,sigma1,sigma2,rho)
cor(xy_cpp[,1],xy_cpp[,2])
```  

## Answer2)
```{r}
N=5000;mu1=0;mu2=0;sigma1=1;sigma2=1;rho=0.9
gibbsR<-function(N=5000,mu1=0,mu2=0,sigma1=1,sigma2=1,rho=0.9){
  xy<-matrix(0,N,2)
  xy[1,]<-c(mu1,mu2)
  s1 <- sqrt(1-rho^2)*sigma1
  s2 <- sqrt(1-rho^2)*sigma2
  xy[1,]<-c(mu1,mu2)
  for (i in 2:N) {
    x2 <- xy[i-1, 2]
    m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
    xy[i, 1] <- rnorm(1, m1, s1)
    x1 <- xy[i, 1]
    m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
    xy[i, 2] <- rnorm(1, m2, s2)
  }
  return(xy)
}
xy<-gibbsR()
cor(xy[,1],xy[,2])
qqplot(xy_cpp,xy)
```  







